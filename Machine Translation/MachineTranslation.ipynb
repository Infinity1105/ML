{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Machine Translation\n",
    "1. This is a Machine Traslation code.<br>\n",
    "2. This code is a novice machine translation code which translates the <font color=#068DA9>English word to French word</font>.<br>\n",
    "3. I used the concepts Transformation matrix. I optimised this transformation by reducing the loss function which is calculated using Forbinous Norm.<br>\n",
    "4. I used <font color=#068DA9><b>K-Nearest neighnour</b></font> to find the closest similar french word.Similrity parameter used in this implemenatation is <b>cosine similarity</b>.<br> \n",
    "5. I used the subset of Google's word2VEC model for finding the word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import scipy\n",
    "import pickle\n",
    "import pdb  #python  debugger\n",
    "import sklearn\n",
    "\n",
    "import gensim\n",
    "# Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "# Target audience is the natural language processing (NLP) and information retrieval (IR) community.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords,twitter_samples\n",
    "from os import getcwd\n",
    "from gensim.models import keyedvectors\n",
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The data\n",
    "en_embedding_subset=pickle.load(open(\"en_embeddings.p\",\"rb\"))\n",
    "fr_embedding_subset=pickle.load(open(\"fr_embeddings.p\",\"rb\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data\n",
    "en_embeddings_subset: the key is an English word, and the vaule is a\n",
    "300 dimensional array, which is the embedding for that word.\n",
    "\n",
    "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
    "fr_embeddings_subset: the key is an French word, and the vaule is a 300\n",
    "dimensional array, which is the embedding for that word.\n",
    "\n",
    "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "from utils import get_dict,consine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the training french dictionary is  5000\n",
      "length of the testing french dictionary is  1500\n"
     ]
    }
   ],
   "source": [
    "en_fr_train=get_dict('en-fr.train.txt')\n",
    "print('length of the training french dictionary is ',len(en_fr_train))\n",
    "en_fr_test=get_dict('en-fr.test.txt')\n",
    "print('length of the testing french dictionary is ',len(en_fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, english_vecs, french_vecs):\n",
    "    x_l=list()\n",
    "    y_l=list()\n",
    "    #Creating sets for \n",
    "    fr_set=set(en_fr.values())\n",
    "    english_word_set = english_vecs.keys()\n",
    "    french_word_set=french_vecs.keys()\n",
    "    for en_word in en_fr.keys():\n",
    "        if (en_word in english_word_set and en_fr[en_word] in french_word_set):\n",
    "            english_word_vec=english_vecs[en_word]\n",
    "            fr_word=en_fr[en_word]\n",
    "            french_word_vec = french_vecs[fr_word]\n",
    "            x_l.append(english_word_vec)\n",
    "            y_l.append(french_word_vec)\n",
    "    X=np.vstack(x_l)\n",
    "    Y=np.vstack(y_l)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X_train is  4932\n",
      "length of y_train is  4932\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train=get_matrices(en_fr_train,en_embedding_subset,fr_embedding_subset)\n",
    "X_test,y_test=get_matrices(en_fr_test,en_embedding_subset,fr_embedding_subset)\n",
    "print(\"length of X_train is \",len(X_train))\n",
    "print(\"length of y_train is \",len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transaltion part in Machine Translation</h3>\n",
    "<font color=#068DA9><h3>Linear Transformation :-</h3></font>\n",
    "<b><em>e*R=f </em><br>\n",
    "now after getting f we look for nearest neighbours of f,the most nearest will be our transalation word</b>\n",
    "<h5 style=\"color:#068DA9>\">The loss function is =square(1/m||XR-Y|<sub>F</sub>|<sup>2</sup>)</h5>\n",
    "where we are taking Forbinious Norm here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.291502622129181"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import linalg as la\n",
    "a=np.array([[1,2,3]\n",
    "           ,[1,2,3]])\n",
    "la.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calulation of loss\n",
    "def loss(X,Y,R):\n",
    "    #here \n",
    "    # X =english word matrix\n",
    "    # Y=French word matrix\n",
    "    # R is Transformation matrix\n",
    "    #value of ||XR-Y||\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector \n",
    "        space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    A=((np.dot(X,R))-Y)\n",
    "    m=len(X)\n",
    "    loss=((la.norm(A))**2)/m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation of Gradient Loss\n",
    "def d_loss(X,Y,R):\n",
    "    m=len(X)\n",
    "    gredient_loss=2*(np.dot(X.transpose(),(np.dot(X,R)-Y)))/m\n",
    "    return gredient_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,alpha=0.01,steps=100):\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    for i in range(0,steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {loss(X, Y, R):.4f}\")\n",
    "        R=R-(d_loss(X,Y,R)*alpha)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 967.5624\n",
      "loss at iteration 25 is: 97.9774\n",
      "loss at iteration 50 is: 26.8542\n",
      "loss at iteration 75 is: 9.7979\n",
      "loss at iteration 100 is: 4.3842\n",
      "loss at iteration 125 is: 2.3338\n",
      "loss at iteration 150 is: 1.4528\n",
      "loss at iteration 175 is: 1.0376\n",
      "loss at iteration 200 is: 0.8281\n",
      "loss at iteration 225 is: 0.7167\n",
      "loss at iteration 250 is: 0.6550\n",
      "loss at iteration 275 is: 0.6197\n",
      "loss at iteration 300 is: 0.5989\n",
      "loss at iteration 325 is: 0.5864\n",
      "loss at iteration 350 is: 0.5787\n",
      "loss at iteration 375 is: 0.5738\n"
     ]
    }
   ],
   "source": [
    "R_train=gradient_descent(X_train,y_train,alpha=0.8,steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(word_vector,candidates,K=1):\n",
    "    similarity_l=list()\n",
    "    for neighbor in candidates:\n",
    "        cos=consine_similarity(word_vector, neighbor)\n",
    "        similarity_l.append(cos)\n",
    "    sorted_k=np.argsort(similarity_l)\n",
    "    k_idx=sorted_k[-K:]\n",
    "    return k_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array(\n",
    "    [[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[KNN(v, candidates, 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X,Y,R):\n",
    "    F=np.dot(X,R)\n",
    "    count=0\n",
    "    for i in range(0,len(F)):\n",
    "        predicted=KNN(F[i],Y,K=1)\n",
    "        if predicted==i:\n",
    "            count+=1\n",
    "    \n",
    "    accuracy=count/len(F)*100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.21557719054242\n"
     ]
    }
   ],
   "source": [
    "print(test_vocabulary(X_test,y_test,R_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
